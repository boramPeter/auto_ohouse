import json
import time,sys,os,re
import asyncio,pprint
from contextlib import AsyncExitStack
from typing import List, Dict, Any, Optional
import traceback
import streamlit as st
from dotenv import load_dotenv
import getpass
import tempfile
from datetime import datetime, timedelta
from qa_mcp_server.figma.get_data_figma import FIGMA_TOKEN_CREATION_DATE
import uuid


class DynamicMCPClient:
    def __init__(self, server_configs: List[Dict[str, Any]]):
        self.server_configs = server_configs
        self.openai = None

        # API í‚¤ í™•ì¸ ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if GOOGLE_API_KEY:
            try:
                self.openai = OpenAI(
                    api_key=GOOGLE_API_KEY,
                    base_url="https://generativelanguage.googleapis.com/v1beta/",
                )
                print("OpenAI (Google AI) í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as e:
                 # ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ Streamlit UIì— ì˜¤ë¥˜ í‘œì‹œ (ì•± ì‹œì‘ ì‹œ)
                 st.error(f"ğŸš¨ OpenAI (Google AI) í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                 print(f"OpenAI (Google AI) í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        else:
            # API í‚¤ ëˆ„ë½ ì‹œ Streamlit UIì— ì˜¤ë¥˜ í‘œì‹œ (ì•± ì‹œì‘ ì‹œ)
            # ì…ë ¥ í•„ë“œëŠ” ì‚¬ì´ë“œë°”ì—ì„œ ì²˜ë¦¬
            print("ê²½ê³ : Google AI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            # self.openai ëŠ” None ìƒíƒœë¡œ ìœ ì§€ë¨

    async def process_query(self,
                            original_user_query: str,  # ëª…í™•ì„±ì„ ìœ„í•´ ë³€ìˆ˜ëª… ë³€ê²½
                            chat_history: List[Dict[str, Any]],
                            status_placeholder: Optional[Any] = None  # DeltaGenerator ëŒ€ì‹  Any
                            ) -> Dict[str, Any]:
        # --- OpenAI í´ë¼ì´ì–¸íŠ¸ ë° ì„¤ì •ê°’ í™•ì¸ (ì‹¤ì œ __init__ ë¡œì§ í•„ìš”) ---
        if not hasattr(self, 'openai') or not self.openai:
            print("ê²½ê³ : OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ self.openaië¡œ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (process_query)")
            return {"final_text": "ì˜¤ë¥˜: Google AI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "tool_status": []}

        # MODEL_NAME, MAX_TOKENSëŠ” í´ë˜ìŠ¤ ë©¤ë²„ ë˜ëŠ” ì „ì—­ ë³€ìˆ˜ë¡œ ì ‘ê·¼ ê°€ëŠ¥í•´ì•¼ í•¨
        # ì´ ì˜ˆì‹œì—ì„œëŠ” ì „ì—­ ìƒìˆ˜ë¥¼ ì‚¬ìš©í•œë‹¤ê³  ê°€ì • (ì‹¤ì œ ì½”ë“œì— ë§ê²Œ ì¡°ì •)
        # global MODEL_NAME, MAX_TOKENS

        print("--- DynamicMCPClient.process_query ì‹œì‘ë¨ ---")
        print(f"í˜„ì¬ ë¡œë“œëœ ì„œë²„ ì„¤ì • (self.server_configs): {self.server_configs}")

        # ìµœì¢… ë°˜í™˜ë  LLM í…ìŠ¤íŠ¸ ë° ëˆ„ì ëœ ë„êµ¬ ìƒíƒœ ë©”ì‹œì§€
        final_llm_conclusion_text = "[LLM ìµœì¢… ê²°ë¡  ì—†ìŒ]"
        accumulated_tool_status_messages: List[str] = []

        sessions: Dict[str, Any] = {}  # ClientSession
        server_tools_map: Dict[str, List[Any]] = {}  # types.Tool
        connection_errors: List[str] = []

        async with AsyncExitStack() as exit_stack:
            print(f"\n--- AsyncExitStack ì§„ì… ---")
            print(f"--- MCP ì„œë²„ ì—°ê²° ì‹œë„ ({len(self.server_configs)}ê°œ) ---")
            connect_start_time = time.monotonic()

            for config in self.server_configs:
                server_id = config["id"]
                server_params_obj = config["params"]  # StdioServerParameters ê°ì²´

                print(f"  [ë””ë²„ê·¸] ì„œë²„ ID '{server_id}' ì—°ê²° ì‹œë„ ì‹œì‘. íŒŒë¼ë¯¸í„°: {server_params_obj}")
                try:
                    print(f"    [ë””ë²„ê·¸] stdio_client í˜¸ì¶œ ì˜ˆì • ({server_id})...")
                    stdio_transport = await exit_stack.enter_async_context(
                        stdio_client(server_params_obj)  # ì‹¤ì œ stdio_client í•¨ìˆ˜
                    )
                    print(f"    [ë””ë²„ê·¸] stdio_client í˜¸ì¶œ ì™„ë£Œ ({server_id})")

                    print(f"    [ë””ë²„ê·¸] ClientSession ìƒì„± ì˜ˆì • ({server_id})...")
                    session = await exit_stack.enter_async_context(
                        ClientSession(*stdio_transport)  # ì‹¤ì œ ClientSession í´ë˜ìŠ¤
                    )
                    print(f"    [ë””ë²„ê·¸] ClientSession ìƒì„± ì™„ë£Œ ({server_id})")

                    print(f"    [ë””ë²„ê·¸] session.initialize() í˜¸ì¶œ ì˜ˆì • ({server_id})...")
                    await asyncio.wait_for(session.initialize(), timeout=120.0)
                    print(f"    [ë””ë²„ê·¸] session.initialize() ì™„ë£Œ ({server_id}).")

                    print(f"    [ë””ë²„ê·¸] session.list_tools() í˜¸ì¶œ ì˜ˆì • ({server_id})...")
                    response = await asyncio.wait_for(session.list_tools(), timeout=60.0)
                    print(f"    [ë””ë²„ê·¸] session.list_tools() ì™„ë£Œ ({server_id}).")

                    sessions[server_id] = session
                    server_tools_map[server_id] = response.tools
                    print(f"  âœ… '{server_id}' ì—°ê²° ì„±ê³µ. MCP Server: {[tool.name for tool in response.tools]}")

                except asyncio.TimeoutError:
                    err_msg = f"  âš ï¸ '{server_id}' ì—°ê²° ì‹œê°„ ì´ˆê³¼."
                    print(err_msg)
                    connection_errors.append(err_msg)
                except Exception as e:
                    err_msg = f"  âŒ '{server_id}' ì—°ê²° ì‹¤íŒ¨: {e}"
                    print(err_msg)
                    print(f"  [ë””ë²„ê·¸] ìƒì„¸ íŠ¸ë ˆì´ìŠ¤ë°± ({server_id}):")
                    traceback.print_exc()
                    connection_errors.append(err_msg)
                print(f"  [ë””ë²„ê·¸] ì„œë²„ ID '{server_id}' ì—°ê²° ì‹œë„ ì¢…ë£Œ.")

            connect_end_time = time.monotonic()
            print(f"--- ì„œë²„ ì—°ê²° ì‹œë„ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {connect_end_time - connect_start_time:.2f}ì´ˆ) ---")
            print(f"--- ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ëœ ì„œë²„: {list(sessions.keys())} ---")

            if 'st' in sys.modules:  # Streamlit ì»¨í…ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                detected_tool_names = []
                for sid, tool_list in server_tools_map.items():
                    for tool in tool_list:
                        detected_tool_names.append(f"{sid}-{tool.name}".replace("_", "-"))
                st.session_state.current_available_tools = detected_tool_names
                print(f"--- Streamlit ì„¸ì…˜ì— ì‚¬ìš© ê°€ëŠ¥ ë„êµ¬ ì €ì¥: {detected_tool_names} ---")

            if not sessions:
                final_llm_conclusion_text = "ì˜¤ë¥˜: MCP Server ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                if 'st' in sys.modules:
                    st.error(final_llm_conclusion_text + "\n" + "\n".join(connection_errors))
                return {"final_text": final_llm_conclusion_text, "tool_status": []}

            # --- ë°˜ë³µì  LLM í˜¸ì¶œ ë° ë„êµ¬ ì‹¤í–‰ ë¡œì§ ---
            max_iterations = 20  # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ (ë„ˆë¬´ ë§ìœ¼ë©´ ë¬´í•œë£¨í”„ ê°€ëŠ¥ì„±)
            current_iteration = 0
            task_accomplished_by_llm = False

            # LLMê³¼ì˜ ëŒ€í™” ê¸°ë¡ (ì´ë²ˆ process_query ë‚´ì—ì„œë§Œ ì‚¬ìš©)
            # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜•ì‹ì˜ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            iteration_messages: List[Dict[str, Any]] = []
            last_tool_result_content = None # ë§ˆì§€ë§‰ ë„êµ¬ ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”

            # ì´ˆê¸° ì‚¬ìš©ì ìš”ì²­ êµ¬ì„± (OpenAI í˜•ì‹)
            # chat_historyì˜ ê° ë©”ì‹œì§€ contentê°€ Noneì´ ì•„ë‹ˆë„ë¡ ì²˜ë¦¬
            for i, hist_msg in enumerate(chat_history):
                role = hist_msg.get("role")
                content = hist_msg.get("content")
                tool_calls = hist_msg.get("tool_calls")

                if role == "tool":  # ë„êµ¬ ì‘ë‹µì€ ì•„ë˜ ë¡œì§ì—ì„œ iteration_messagesì— ì¶”ê°€ë¨
                    continue

                message_to_add = {"role": role}
                processed_content = content

                if not (role == "assistant" and tool_calls):  # ë„êµ¬ í˜¸ì¶œì´ ì—†ëŠ” ì‚¬ìš©ì ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€
                    if processed_content is None or (
                            isinstance(processed_content, str) and not processed_content.strip()):
                        processed_content = " "  # ë¹ˆ ì½˜í…ì¸ ëŠ” ê³µë°±ìœ¼ë¡œ (íŠ¹íˆ ì²« ë©”ì‹œì§€)

                if processed_content is not None:
                    message_to_add["content"] = processed_content

                if role == "assistant" and tool_calls:
                    message_to_add["tool_calls"] = tool_calls
                    if content is None:  # ëª…ì‹œì ìœ¼ë¡œ contentê°€ Noneì´ì—ˆë˜ ê²½ìš°
                        message_to_add.pop("content", None)

                if "content" not in message_to_add and "tool_calls" not in message_to_add:
                    if role == "user" or role == "assistant":
                        message_to_add["content"] = " "  # ì•ˆì „ì¥ì¹˜

                iteration_messages.append(message_to_add)

            # í˜„ì¬ ì‚¬ìš©ì ì›ë³¸ ì¿¼ë¦¬ ì¶”ê°€ (OpenAI í˜•ì‹)
            current_query_text_for_llm = original_user_query
            if not iteration_messages:  # ì²« ë©”ì‹œì§€ì¸ ê²½ìš°
                if not current_query_text_for_llm or not current_query_text_for_llm.strip():
                    current_query_text_for_llm = " "
            elif current_query_text_for_llm is None:  # ì²« ë©”ì‹œì§€ëŠ” ì•„ë‹ˆì§€ë§Œ Noneì¸ ê²½ìš°
                current_query_text_for_llm = " "

            iteration_messages.append({"role": "user", "content": current_query_text_for_llm})

            available_tools_for_llm = self._get_combined_tools_for_llm(server_tools_map)

            try:
                while current_iteration < max_iterations and not task_accomplished_by_llm:
                    current_iteration += 1
                    print(f"\n--- ë°˜ë³µ ì‹¤í–‰ {current_iteration}/{max_iterations} ---")
                    if status_placeholder:
                        status_placeholder.info(f"ğŸ¤– ìƒê° ì¤‘... (ë‹¨ê³„ {current_iteration}/{max_iterations})")

                    # LLM í˜¸ì¶œ: ë‹¤ìŒ í–‰ë™ ê²°ì •
                    print(f"LLM ({current_iteration}ì°¨) í˜¸ì¶œ ë©”ì‹œì§€ ê°œìˆ˜: {len(iteration_messages)}")
                    # pprint.pprint(iteration_messages) # í•„ìš”ì‹œ ìƒì„¸ ë¡œê·¸

                    llm_response_obj = self.openai.chat.completions.create(
                        model=MODEL_NAME,  # ë˜ëŠ” self.MODEL_NAME
                        max_tokens=MAX_TOKENS,  # ë˜ëŠ” self.MAX_TOKENS
                        messages=iteration_messages,
                        tools=available_tools_for_llm if available_tools_for_llm else None,
                    )
                    llm_message_from_response = llm_response_obj.choices[0].message

                    # LLM ì‘ë‹µì„ ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•´ ê¸°ë¡ (OpenAI í˜•ì‹ ê·¸ëŒ€ë¡œ)
                    iteration_messages.append(llm_message_from_response.model_dump(exclude_none=True))

                    if not llm_message_from_response.tool_calls:
                        # LLMì´ ë„êµ¬ í˜¸ì¶œ ì—†ì´ ì§ì ‘ ë‹µë³€ -> ì‘ì—… ì™„ë£Œë¡œ ê°„ì£¼
                        print("LLMì´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤ (ë„êµ¬ í˜¸ì¶œ ì—†ìŒ).")
                        final_llm_conclusion_text = llm_message_from_response.content if llm_message_from_response.content else "ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                        task_accomplished_by_llm = True
                        break  # ë°˜ë³µ ì¢…ë£Œ
                    requested_tool_calls = llm_message_from_response.tool_calls
                    print(f"LLM ìš”ì²­ ë„êµ¬: {[tc.function.name for tc in requested_tool_calls]}")
                    if status_placeholder:
                        summary = [f"â³ [{tc.function.name}, ì¸ì:{tc.function.arguments}]" for tc in requested_tool_calls]
                        status_placeholder.info(f"ë‹¨ê³„ {current_iteration}: MCP server ì‹¤í–‰ ì¤‘\n" + "\n".join(summary))

                    tool_tasks_map = {}
                    tool_results_map = {}

                    # 1. ì‹¤í–‰í•  íƒœìŠ¤í¬ ì¤€ë¹„ ë˜ëŠ” ì¦‰ì‹œ ì˜¤ë¥˜ ì²˜ë¦¬
                    for tool_call in requested_tool_calls:
                        try:
                            global TOOL_ARGS  # ì „ì—­ ë³€ìˆ˜ ì‚¬ìš© ì„ ì–¸
                            TOOL_ARGS = tool_call.function.arguments

                            tool_args = json.loads(tool_call.function.arguments)
                            task = self._find_and_call_tool(tool_call.function.name, tool_args, sessions,
                                                            server_tools_map)
                            tool_tasks_map[tool_call.id] = asyncio.create_task(task)
                        except Exception as e:
                            print(f"ì˜¤ë¥˜: MCP Server '{tool_call.function.name}' í˜¸ì¶œ ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜: {e}")
                            error_content = json.dumps(
                                {"error": f"Tool call preparation or argument parsing error: {e}"})
                            tool_results_map[tool_call.id] = {"role": "tool", "tool_call_id": tool_call.id,
                                                              "name": tool_call.function.name, "content": error_content}
                            accumulated_tool_status_messages.append(f"âš ï¸ [{tool_call.function.name}] í˜¸ì¶œ ì¤€ë¹„ ì˜¤ë¥˜")

                    # 2. ì¤€ë¹„ëœ íƒœìŠ¤í¬ë“¤ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰
                    if tool_tasks_map:
                        task_items = list(tool_tasks_map.items())
                        executed_results = await asyncio.gather(*[task for _, task in task_items],
                                                                return_exceptions=True)

                        # 3. ì‹¤í–‰ëœ ê²°ê³¼ë¥¼ ì›ë˜ì˜ tool_call_idì™€ ë§¤í•‘
                        for i, (tool_call_id, _) in enumerate(task_items):
                            result_obj = executed_results[i]
                            original_tool_call = next((tc for tc in requested_tool_calls if tc.id == tool_call_id),
                                                      None)
                            status_msg_for_ui = ""
                            result_content_for_llm = ""

                            if isinstance(result_obj, Exception):
                                print(f"!!! MCP Server ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜(gather): {result_obj}")
                                error_text = json.dumps({"error": f"Tool execution error: {str(result_obj)}"})
                                result_content_for_llm = error_text
                                status_msg_for_ui = f"âŒ [{original_tool_call.function.name}] ì‹¤í–‰ ì˜¤ë¥˜"
                            elif isinstance(result_obj, types.CallToolResult):
                                res_text = "[ê²°ê³¼ ì—†ìŒ]"
                                if result_obj.content and result_obj.content[0] and isinstance(result_obj.content[0],
                                                                                               types.TextContent):
                                    res_text = result_obj.content[0].text
                                result_content_for_llm = res_text
                                status_msg_for_ui = f"âœ… [{original_tool_call.function.name}] ì‹¤í–‰ ì™„ë£Œ: {res_text[:100].replace(os.linesep, ' ')}..."
                                last_tool_result_content = result_content_for_llm
                            else:
                                error_text = json.dumps({"error": f"Unexpected result type: {type(result_obj)}"})
                                result_content_for_llm = error_text
                                status_msg_for_ui = f"â“ [{original_tool_call.function.name}] ì•Œ ìˆ˜ ì—†ëŠ” ê²°ê³¼ íƒ€ì…"

                            tool_results_map[tool_call_id] = {"role": "tool", "tool_call_id": tool_call_id,
                                                              "name": original_tool_call.function.name,
                                                              "content": result_content_for_llm}
                            accumulated_tool_status_messages.append(status_msg_for_ui)

                    # 4. ì›ë˜ ìš”ì²­ ìˆœì„œëŒ€ë¡œ ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
                    tool_results_for_this_iteration = [tool_results_map[tc.id] for tc in requested_tool_calls]
                    iteration_messages.extend(tool_results_for_this_iteration)

                    # ë£¨í”„ì˜ ë, ë‹¤ìŒ ë°˜ë³µìœ¼ë¡œ
                    if task_accomplished_by_llm:  # LLMì´ ìŠ¤ìŠ¤ë¡œ ì™„ë£Œë¥¼ ì„ ì–¸í•œ ê²½ìš°
                        print(f"LLMì´ ë°˜ë³µ {current_iteration}ì—ì„œ ì‘ì—… ì™„ë£Œë¥¼ ì„ ì–¸í–ˆìŠµë‹ˆë‹¤.")
                        break

            except Exception as processing_error:
                print(f"!!! ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ (ë°˜ë³µ ë£¨í”„ ë‚´): {processing_error}")
                traceback.print_exc()
                final_llm_conclusion_text = f"[ì˜¤ë¥˜ ë°œìƒ: {processing_error}]"
                task_accomplished_by_llm = True  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë” ì´ìƒ ì§„í–‰í•˜ì§€ ì•ŠìŒ

            # --- ë£¨í”„ ì¢…ë£Œ í›„ ---
            if not task_accomplished_by_llm:
                if current_iteration >= max_iterations:
                    print("ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. LLMì—ê²Œ ìš”ì•½ì„ ìš”ì²­í•©ë‹ˆë‹¤.")

                    # --- ìµœì¢… ìš”ì•½ì„ ìœ„í•œ ì¶”ê°€ LLM í˜¸ì¶œ ---
                    # í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©ì— ìš”ì•½ì„ ìš”ì²­í•˜ëŠ” ì§€ì¹¨ ì¶”ê°€
                    summarization_prompt = (
                        "You have now reached the maximum number of intermediate steps. "
                        "Please provide a final, conclusive answer to the user based on the conversation history so far. "
                        "Summarize what you have done and what the result is. "
                        "Do not call any more tools, just provide the final text response."
                        "ê·¸ë¦¬ê³  í•œê¸€ë¡œ ë‹µë³€í•´ì¤˜."
                    )
                    iteration_messages.append({"role": "user", "content": summarization_prompt})

                    try:
                        # ìš”ì•½ì„ ìœ„í•œ ë§ˆì§€ë§‰ LLM í˜¸ì¶œ (ë„êµ¬ ì‚¬ìš© ë¹„í™œì„±í™”)
                        final_summary_response = self.openai.chat.completions.create(
                            model=MODEL_NAME,  # ë˜ëŠ” self.MODEL_NAME
                            max_tokens=MAX_TOKENS,  # ë˜ëŠ” self.MAX_TOKENS
                            messages=iteration_messages,
                            tools=None,  # ë” ì´ìƒ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ëª»í•˜ë„ë¡ ì„¤ì •
                        )

                        final_llm_conclusion_text = final_summary_response.choices[0].message.content

                        if not final_llm_conclusion_text:
                            final_llm_conclusion_text = "[ì‘ì—… ìš”ì•½ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.]"

                    except Exception as summary_err:
                        print(f"!!! ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {summary_err}")
                        traceback.print_exc()
                        final_llm_conclusion_text = "[ìµœëŒ€ ë°˜ë³µ ë„ë‹¬ í›„ ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.]"

                else:  # ë‹¤ë¥¸ ì´ìœ ë¡œ ë£¨í”„ê°€ ì¢…ë£Œë˜ì—ˆìœ¼ë‚˜ ëª…ì‹œì  ì™„ë£Œê°€ ì•„ë‹˜ (ì´ ê²½ìš°ëŠ” ê±°ì˜ ë°œìƒí•˜ì§€ ì•ŠìŒ)
                    final_llm_conclusion_text = "[ì‘ì—…ì´ ëª…í™•íˆ ì™„ë£Œë˜ì§€ ì•Šê³  ì¢…ë£Œë¨]"

            if status_placeholder:
                status_placeholder.empty()
            if final_llm_conclusion_text and final_llm_conclusion_text.startswith(
                    "[") and final_llm_conclusion_text.endswith("]"):
                if last_tool_result_content:
                    print("LLMì˜ ìµœì¢… ê²°ë¡ ì´ ì—†ì–´ ë§ˆì§€ë§‰ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ìµœì¢… ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    final_llm_conclusion_text = last_tool_result_content
            if TOOL_NAME in ("auto_tc_maker", "auto_test_scope_func"):
                final_llm_conclusion_text = result_content_for_llm

            print("\n--- ìµœì¢… ë°˜í™˜ë  ë‚´ìš© ---")
            print(f"  final_text content: {repr(final_llm_conclusion_text)}")
            print(f"  tool_status messages count: {len(accumulated_tool_status_messages)}")
            print("--- ìµœì¢… ë°˜í™˜ë  ë‚´ìš© ë ---\n")

            return {
                "final_text": final_llm_conclusion_text,
                "tool_status": accumulated_tool_status_messages
            }


def main_dr_oh_page(key):
    if 'session_id' not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())
    session_id = st.session_state.session_id

    st.markdown(
        """
        <h1 style='display: flex; align-items: center; gap: 10px;'>
            <img src='https://dszw1qtcnsa5e.cloudfront.net/community/20211023/5b80e92c-1b5d-4377-ab6c-cd0a73a75352/6e8b3aa55a5fbaa1a210a8b92bd559e7.jpg' alt='icon' width='80'/>
            ì˜¤ë°•ì‚¬
        </h1>
        """,
        unsafe_allow_html=True
    )

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ì±„íŒ… ê¸°ë¡ë§Œ ìœ ì§€)
    if 'messages' not in st.session_state:
        st.session_state.messages = [] # ì±„íŒ… ê¸°ë¡ ì €ì¥


    # --- ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ---
    st.header(f"ğŸ’¬ ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì´ë“  ìš”ì²­í•˜ì„¸ìš”!")
    print(f"\n--- ì±„íŒ… ê¸°ë¡ í‘œì‹œ ì‹œì‘ ---{session_id}")
    from qa_mcp_server.slack_func.send_slack_msg import send_slack_message
    send_slack_message(f"`ì˜¤ë°•ì‚¬ ì±„íŒ… ê¸°ë¡ ì‹œì‘ {session_id}`")

    print(f"st.session_state.messages ë‚´ìš© (í‘œì‹œ ì§ì „,{session_id}):")
    try:
        pprint.pprint(st.session_state.get('messages', 'messages í‚¤ ì—†ìŒ'))
        send_slack_message(f"```ì˜¤ë°•ì‚¬ ì±„íŒ… ê¸°ë¡ : {st.session_state.get('messages', 'messages í‚¤ ì—†ìŒ')},{session_id}```")
    except Exception as pp_err:
          print(f"(pprint ì˜¤ë¥˜: {pp_err}), Raw: {st.session_state.get('messages', 'messages í‚¤ ì—†ìŒ')}")
          send_slack_message(f"```ì˜¤ë°•ì‚¬ ì±„íŒ… ì˜¤ë¥˜ {pp_err}: {st.session_state.get('messages', 'messages í‚¤ ì—†ìŒ')},{session_id}```")

    print(f"--- ì±„íŒ… ê¸°ë¡ í‘œì‹œ ì „ ë¡œê·¸ ë {session_id} ---\n")
    send_slack_message(f"`ì˜¤ë°•ì‚¬ ì±„íŒ… ê¸°ë¡ ì™„ë£Œ {session_id}`")

    IMAGE_URL_PATTERN = re.compile(r'(https?://\S+\.(?:png|jpe?g|gif|webp|bmp|svg))', re.IGNORECASE)


    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "upload_counter" not in st.session_state:
        st.session_state.upload_counter = 0 # ì¹´ìš´í„° ì´ˆê¸°í™”

    uploader_key = f"image_uploader_{st.session_state.upload_counter}" # ë™ì  í‚¤ ìƒì„±

    st.markdown(
        """
        <style>
        .custom-label {
            font-size: 16px;
            font-weight: 600;
            margin-bottom: 0px;
            margin-top: 0px;
            color: #ffa500;
        }
        .custom-label + div {
            margin-top: 0px !important;
        }
        </style>
        """,
        unsafe_allow_html=True
    )
    # ì‚¬ìš©ì ì •ì˜ ìŠ¤íƒ€ì¼ë¡œ ë¼ë²¨ ì¶œë ¥
    st.markdown('<div class="custom-label"> ğŸ“  ì´ë¯¸ì§€ë¥¼ ì²¨ë¶€í•˜ì„¸ìš”. ìµœëŒ€ 10ì¥ê¹Œì§€ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.</div>', unsafe_allow_html=True)

    uploaded_files = st.file_uploader(
        label="",  # ë¼ë²¨ ëŒ€ì‹  ìœ„ì˜ HTML ì‚¬ìš©
        type=["png", "jpg", "jpeg", "gif", "webp", "bmp"],
        accept_multiple_files=True,
        key=uploader_key,
        help="ìµœëŒ€ 10ì¥ê¹Œì§€ë§Œ ì²˜ë¦¬í•˜ë©°, ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

    # API í‚¤ê°€ ìˆì–´ì•¼ë§Œ ì…ë ¥ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
    chat_input_disabled = not key

    if prompt := st.chat_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...", disabled=chat_input_disabled, key="chat_input_main"):


        user_content_list = []
        image_paths = []  # ë°±ì—”ë“œ ì „ë‹¬ìš© ì´ë¯¸ì§€ ë³€ìˆ˜

        # ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬
        MAX_FILES = 10

if __name__ == "__main__":
    main_dr_oh_page()


